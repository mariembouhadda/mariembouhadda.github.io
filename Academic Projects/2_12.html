<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Mariem Bouhadda" />


<title>Binary Classification of Breast Cancer</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; background-color: #f8f8f8; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
pre, code { background-color: #f8f8f8; }
code > span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code > span.dt { color: #204a87; } /* DataType */
code > span.dv { color: #0000cf; } /* DecVal */
code > span.bn { color: #0000cf; } /* BaseN */
code > span.fl { color: #0000cf; } /* Float */
code > span.ch { color: #4e9a06; } /* Char */
code > span.st { color: #4e9a06; } /* String */
code > span.co { color: #8f5902; font-style: italic; } /* Comment */
code > span.ot { color: #8f5902; } /* Other */
code > span.al { color: #ef2929; } /* Alert */
code > span.fu { color: #000000; } /* Function */
code > span.er { color: #a40000; font-weight: bold; } /* Error */
code > span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #000000; } /* Constant */
code > span.sc { color: #000000; } /* SpecialChar */
code > span.vs { color: #4e9a06; } /* VerbatimString */
code > span.ss { color: #4e9a06; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #000000; } /* Variable */
code > span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code > span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code > span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code > span.ex { } /* Extension */
code > span.at { color: #c4a000; } /* Attribute */
code > span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code > span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Mariem Bouhadda</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="https://mariembouhadda.github.io/">
    <span class="fa fa-home"></span>
     
    Home
  </a>
</li>
<li>
  <a href="2_12.html">Breast Cancer Classification</a>
</li>
<li>
  <a href="arch.html">Stock market volatility</a>
</li>
<li>
  <a href="map.html">Maps</a>
</li>
<li>
  <a href="mariemcv.html">My CV</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="contact.html">
    <span class="fa fa-envelope fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://gist.github.com/mariem1bouhadda">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/mariem-bouhadda/">
    <span class="fa fa-linkedin fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Binary Classification of Breast Cancer</h1>
<h4 class="author"><em>Mariem Bouhadda</em></h4>
<h4 class="date"><em>December, 2018</em></h4>

</div>

<div id="TOC">
<ul>
<li><a href="#data-description-and-summary">1. Data Description and summary</a><ul>
<li><a href="#data-description">1-1) Data description</a></li>
<li><a href="#structure-and-summary">1-2) Structure and summary</a></li>
</ul></li>
<li><a href="#correlation-and-relationship-between-variables">2. Correlation and relationship between variables</a><ul>
<li><a href="#mean">Mean</a></li>
<li><a href="#se">SE</a></li>
<li><a href="#worst">Worst</a></li>
</ul></li>
<li><a href="#principal-component-analysis-pca">3. Principal Component Analysis (PCA)</a><ul>
<li><a href="#screeplot">3-1) Screeplot</a></li>
<li><a href="#see-the-biplot">3-2) See the Biplot</a></li>
</ul></li>
<li><a href="#apply-ml-methods-and-compare-each-other-and-choose-best-fits">4. Apply ML methods and compare each other and choose best fits</a><ul>
<li><a href="#make-test-train-dataset-for-testing-classification-ml-methods">4-1) Make test &amp; train dataset for testing classification ML methods</a></li>
<li><a href="#apply-every-ml-methods-to-data">4-2) Apply every ML methods to data</a></li>
<li><a href="#visualize-to-compare-the-accuracy-of-all-methods">4-3) Visualize to compare the accuracy of all methods</a></li>
</ul></li>
<li><a href="#conclusion">5. Conclusion</a></li>
</ul>
</div>

<!-- ## 1. Intro & Purpose -->
<!-- Hello, Kagglers! -->
<!-- This is my first project since I’ve learned R. -->
<!-- I’ve been studying R around a month, still many things to discover and learn. -->
<!-- Big welcome if you suggest to me about new ML techniques.  -->
<!-- It will be amazing to hear the different ideas to solve the problems with various perspective! -->
<!-- In summary, I used many classification methods in this kernel. -->
<!-- I hope this kernel will be helpful to beginner in this area!  -->
<!-- For better understanding each function, I’ve wrote library right above the function. -->
<hr />
<div id="data-description-and-summary" class="section level2">
<h2>1. Data Description and summary</h2>
<!-- ## 2. Data Importing & Cleaning & Inspecting -->
<div id="data-description" class="section level3">
<h3>1-1) Data description</h3>
<!-- ### 2-1) Import dataset -->
<!-- wbcd means 'wisconsin breast cancer data' -->
<!-- ### 2-2) Remove NULL Data -->
<!-- ### 2-3) Reshape the datasets -->
</div>
<div id="structure-and-summary" class="section level3 tabset">
<h3>1-2) Structure and summary</h3>
<!-- ### 2-4) Inspect the datasets {.tabset} -->
<div id="structure" class="section level4">
<h4>structure</h4>
<pre><code>## &#39;data.frame&#39;:    569 obs. of  31 variables:
##  $ diagnosis              : Factor w/ 2 levels &quot;Benign&quot;,&quot;Malignant&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ radius_mean            : num  18 20.6 19.7 11.4 20.3 ...
##  $ texture_mean           : num  10.4 17.8 21.2 20.4 14.3 ...
##  $ perimeter_mean         : num  122.8 132.9 130 77.6 135.1 ...
##  $ area_mean              : num  1001 1326 1203 386 1297 ...
##  $ smoothness_mean        : num  0.1184 0.0847 0.1096 0.1425 0.1003 ...
##  $ compactness_mean       : num  0.2776 0.0786 0.1599 0.2839 0.1328 ...
##  $ concavity_mean         : num  0.3001 0.0869 0.1974 0.2414 0.198 ...
##  $ concave.points_mean    : num  0.1471 0.0702 0.1279 0.1052 0.1043 ...
##  $ symmetry_mean          : num  0.242 0.181 0.207 0.26 0.181 ...
##  $ fractal_dimension_mean : num  0.0787 0.0567 0.06 0.0974 0.0588 ...
##  $ radius_se              : num  1.095 0.543 0.746 0.496 0.757 ...
##  $ texture_se             : num  0.905 0.734 0.787 1.156 0.781 ...
##  $ perimeter_se           : num  8.59 3.4 4.58 3.44 5.44 ...
##  $ area_se                : num  153.4 74.1 94 27.2 94.4 ...
##  $ smoothness_se          : num  0.0064 0.00522 0.00615 0.00911 0.01149 ...
##  $ compactness_se         : num  0.049 0.0131 0.0401 0.0746 0.0246 ...
##  $ concavity_se           : num  0.0537 0.0186 0.0383 0.0566 0.0569 ...
##  $ concave.points_se      : num  0.0159 0.0134 0.0206 0.0187 0.0188 ...
##  $ symmetry_se            : num  0.03 0.0139 0.0225 0.0596 0.0176 ...
##  $ fractal_dimension_se   : num  0.00619 0.00353 0.00457 0.00921 0.00511 ...
##  $ radius_worst           : num  25.4 25 23.6 14.9 22.5 ...
##  $ texture_worst          : num  17.3 23.4 25.5 26.5 16.7 ...
##  $ perimeter_worst        : num  184.6 158.8 152.5 98.9 152.2 ...
##  $ area_worst             : num  2019 1956 1709 568 1575 ...
##  $ smoothness_worst       : num  0.162 0.124 0.144 0.21 0.137 ...
##  $ compactness_worst      : num  0.666 0.187 0.424 0.866 0.205 ...
##  $ concavity_worst        : num  0.712 0.242 0.45 0.687 0.4 ...
##  $ concave.points_worst   : num  0.265 0.186 0.243 0.258 0.163 ...
##  $ symmetry_worst         : num  0.46 0.275 0.361 0.664 0.236 ...
##  $ fractal_dimension_worst: num  0.1189 0.089 0.0876 0.173 0.0768 ...</code></pre>
</div>
<div id="summary" class="section level4">
<h4>summary</h4>
<pre><code>##      diagnosis    radius_mean      texture_mean   perimeter_mean  
##  Benign   :357   Min.   : 6.981   Min.   : 9.71   Min.   : 43.79  
##  Malignant:212   1st Qu.:11.700   1st Qu.:16.17   1st Qu.: 75.17  
##                  Median :13.370   Median :18.84   Median : 86.24  
##                  Mean   :14.127   Mean   :19.29   Mean   : 91.97  
##                  3rd Qu.:15.780   3rd Qu.:21.80   3rd Qu.:104.10  
##                  Max.   :28.110   Max.   :39.28   Max.   :188.50  
##    area_mean      smoothness_mean   compactness_mean  concavity_mean   
##  Min.   : 143.5   Min.   :0.05263   Min.   :0.01938   Min.   :0.00000  
##  1st Qu.: 420.3   1st Qu.:0.08637   1st Qu.:0.06492   1st Qu.:0.02956  
##  Median : 551.1   Median :0.09587   Median :0.09263   Median :0.06154  
##  Mean   : 654.9   Mean   :0.09636   Mean   :0.10434   Mean   :0.08880  
##  3rd Qu.: 782.7   3rd Qu.:0.10530   3rd Qu.:0.13040   3rd Qu.:0.13070  
##  Max.   :2501.0   Max.   :0.16340   Max.   :0.34540   Max.   :0.42680  
##  concave.points_mean symmetry_mean    fractal_dimension_mean
##  Min.   :0.00000     Min.   :0.1060   Min.   :0.04996       
##  1st Qu.:0.02031     1st Qu.:0.1619   1st Qu.:0.05770       
##  Median :0.03350     Median :0.1792   Median :0.06154       
##  Mean   :0.04892     Mean   :0.1812   Mean   :0.06280       
##  3rd Qu.:0.07400     3rd Qu.:0.1957   3rd Qu.:0.06612       
##  Max.   :0.20120     Max.   :0.3040   Max.   :0.09744       
##    radius_se        texture_se      perimeter_se       area_se       
##  Min.   :0.1115   Min.   :0.3602   Min.   : 0.757   Min.   :  6.802  
##  1st Qu.:0.2324   1st Qu.:0.8339   1st Qu.: 1.606   1st Qu.: 17.850  
##  Median :0.3242   Median :1.1080   Median : 2.287   Median : 24.530  
##  Mean   :0.4052   Mean   :1.2169   Mean   : 2.866   Mean   : 40.337  
##  3rd Qu.:0.4789   3rd Qu.:1.4740   3rd Qu.: 3.357   3rd Qu.: 45.190  
##  Max.   :2.8730   Max.   :4.8850   Max.   :21.980   Max.   :542.200  
##  smoothness_se      compactness_se      concavity_se    
##  Min.   :0.001713   Min.   :0.002252   Min.   :0.00000  
##  1st Qu.:0.005169   1st Qu.:0.013080   1st Qu.:0.01509  
##  Median :0.006380   Median :0.020450   Median :0.02589  
##  Mean   :0.007041   Mean   :0.025478   Mean   :0.03189  
##  3rd Qu.:0.008146   3rd Qu.:0.032450   3rd Qu.:0.04205  
##  Max.   :0.031130   Max.   :0.135400   Max.   :0.39600  
##  concave.points_se   symmetry_se       fractal_dimension_se
##  Min.   :0.000000   Min.   :0.007882   Min.   :0.0008948   
##  1st Qu.:0.007638   1st Qu.:0.015160   1st Qu.:0.0022480   
##  Median :0.010930   Median :0.018730   Median :0.0031870   
##  Mean   :0.011796   Mean   :0.020542   Mean   :0.0037949   
##  3rd Qu.:0.014710   3rd Qu.:0.023480   3rd Qu.:0.0045580   
##  Max.   :0.052790   Max.   :0.078950   Max.   :0.0298400   
##   radius_worst   texture_worst   perimeter_worst    area_worst    
##  Min.   : 7.93   Min.   :12.02   Min.   : 50.41   Min.   : 185.2  
##  1st Qu.:13.01   1st Qu.:21.08   1st Qu.: 84.11   1st Qu.: 515.3  
##  Median :14.97   Median :25.41   Median : 97.66   Median : 686.5  
##  Mean   :16.27   Mean   :25.68   Mean   :107.26   Mean   : 880.6  
##  3rd Qu.:18.79   3rd Qu.:29.72   3rd Qu.:125.40   3rd Qu.:1084.0  
##  Max.   :36.04   Max.   :49.54   Max.   :251.20   Max.   :4254.0  
##  smoothness_worst  compactness_worst concavity_worst  concave.points_worst
##  Min.   :0.07117   Min.   :0.02729   Min.   :0.0000   Min.   :0.00000     
##  1st Qu.:0.11660   1st Qu.:0.14720   1st Qu.:0.1145   1st Qu.:0.06493     
##  Median :0.13130   Median :0.21190   Median :0.2267   Median :0.09993     
##  Mean   :0.13237   Mean   :0.25427   Mean   :0.2722   Mean   :0.11461     
##  3rd Qu.:0.14600   3rd Qu.:0.33910   3rd Qu.:0.3829   3rd Qu.:0.16140     
##  Max.   :0.22260   Max.   :1.05800   Max.   :1.2520   Max.   :0.29100     
##  symmetry_worst   fractal_dimension_worst
##  Min.   :0.1565   Min.   :0.05504        
##  1st Qu.:0.2504   1st Qu.:0.07146        
##  Median :0.2822   Median :0.08004        
##  Mean   :0.2901   Mean   :0.08395        
##  3rd Qu.:0.3179   3rd Qu.:0.09208        
##  Max.   :0.6638   Max.   :0.20750</code></pre>
</div>
<div id="head" class="section level4">
<h4>head</h4>
<table>
<thead>
<tr class="header">
<th align="left">diagnosis</th>
<th align="right">radius_mean</th>
<th align="right">texture_mean</th>
<th align="right">perimeter_mean</th>
<th align="right">area_mean</th>
<th align="right">smoothness_mean</th>
<th align="right">compactness_mean</th>
<th align="right">concavity_mean</th>
<th align="right">concave.points_mean</th>
<th align="right">symmetry_mean</th>
<th align="right">fractal_dimension_mean</th>
<th align="right">radius_se</th>
<th align="right">texture_se</th>
<th align="right">perimeter_se</th>
<th align="right">area_se</th>
<th align="right">smoothness_se</th>
<th align="right">compactness_se</th>
<th align="right">concavity_se</th>
<th align="right">concave.points_se</th>
<th align="right">symmetry_se</th>
<th align="right">fractal_dimension_se</th>
<th align="right">radius_worst</th>
<th align="right">texture_worst</th>
<th align="right">perimeter_worst</th>
<th align="right">area_worst</th>
<th align="right">smoothness_worst</th>
<th align="right">compactness_worst</th>
<th align="right">concavity_worst</th>
<th align="right">concave.points_worst</th>
<th align="right">symmetry_worst</th>
<th align="right">fractal_dimension_worst</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Malignant</td>
<td align="right">17.99</td>
<td align="right">10.38</td>
<td align="right">122.80</td>
<td align="right">1001.0</td>
<td align="right">0.11840</td>
<td align="right">0.27760</td>
<td align="right">0.3001</td>
<td align="right">0.14710</td>
<td align="right">0.2419</td>
<td align="right">0.07871</td>
<td align="right">1.0950</td>
<td align="right">0.9053</td>
<td align="right">8.589</td>
<td align="right">153.40</td>
<td align="right">0.006399</td>
<td align="right">0.04904</td>
<td align="right">0.05373</td>
<td align="right">0.01587</td>
<td align="right">0.03003</td>
<td align="right">0.006193</td>
<td align="right">25.38</td>
<td align="right">17.33</td>
<td align="right">184.60</td>
<td align="right">2019.0</td>
<td align="right">0.1622</td>
<td align="right">0.6656</td>
<td align="right">0.7119</td>
<td align="right">0.2654</td>
<td align="right">0.4601</td>
<td align="right">0.11890</td>
</tr>
<tr class="even">
<td align="left">Malignant</td>
<td align="right">20.57</td>
<td align="right">17.77</td>
<td align="right">132.90</td>
<td align="right">1326.0</td>
<td align="right">0.08474</td>
<td align="right">0.07864</td>
<td align="right">0.0869</td>
<td align="right">0.07017</td>
<td align="right">0.1812</td>
<td align="right">0.05667</td>
<td align="right">0.5435</td>
<td align="right">0.7339</td>
<td align="right">3.398</td>
<td align="right">74.08</td>
<td align="right">0.005225</td>
<td align="right">0.01308</td>
<td align="right">0.01860</td>
<td align="right">0.01340</td>
<td align="right">0.01389</td>
<td align="right">0.003532</td>
<td align="right">24.99</td>
<td align="right">23.41</td>
<td align="right">158.80</td>
<td align="right">1956.0</td>
<td align="right">0.1238</td>
<td align="right">0.1866</td>
<td align="right">0.2416</td>
<td align="right">0.1860</td>
<td align="right">0.2750</td>
<td align="right">0.08902</td>
</tr>
<tr class="odd">
<td align="left">Malignant</td>
<td align="right">19.69</td>
<td align="right">21.25</td>
<td align="right">130.00</td>
<td align="right">1203.0</td>
<td align="right">0.10960</td>
<td align="right">0.15990</td>
<td align="right">0.1974</td>
<td align="right">0.12790</td>
<td align="right">0.2069</td>
<td align="right">0.05999</td>
<td align="right">0.7456</td>
<td align="right">0.7869</td>
<td align="right">4.585</td>
<td align="right">94.03</td>
<td align="right">0.006150</td>
<td align="right">0.04006</td>
<td align="right">0.03832</td>
<td align="right">0.02058</td>
<td align="right">0.02250</td>
<td align="right">0.004571</td>
<td align="right">23.57</td>
<td align="right">25.53</td>
<td align="right">152.50</td>
<td align="right">1709.0</td>
<td align="right">0.1444</td>
<td align="right">0.4245</td>
<td align="right">0.4504</td>
<td align="right">0.2430</td>
<td align="right">0.3613</td>
<td align="right">0.08758</td>
</tr>
<tr class="even">
<td align="left">Malignant</td>
<td align="right">11.42</td>
<td align="right">20.38</td>
<td align="right">77.58</td>
<td align="right">386.1</td>
<td align="right">0.14250</td>
<td align="right">0.28390</td>
<td align="right">0.2414</td>
<td align="right">0.10520</td>
<td align="right">0.2597</td>
<td align="right">0.09744</td>
<td align="right">0.4956</td>
<td align="right">1.1560</td>
<td align="right">3.445</td>
<td align="right">27.23</td>
<td align="right">0.009110</td>
<td align="right">0.07458</td>
<td align="right">0.05661</td>
<td align="right">0.01867</td>
<td align="right">0.05963</td>
<td align="right">0.009208</td>
<td align="right">14.91</td>
<td align="right">26.50</td>
<td align="right">98.87</td>
<td align="right">567.7</td>
<td align="right">0.2098</td>
<td align="right">0.8663</td>
<td align="right">0.6869</td>
<td align="right">0.2575</td>
<td align="right">0.6638</td>
<td align="right">0.17300</td>
</tr>
<tr class="odd">
<td align="left">Malignant</td>
<td align="right">20.29</td>
<td align="right">14.34</td>
<td align="right">135.10</td>
<td align="right">1297.0</td>
<td align="right">0.10030</td>
<td align="right">0.13280</td>
<td align="right">0.1980</td>
<td align="right">0.10430</td>
<td align="right">0.1809</td>
<td align="right">0.05883</td>
<td align="right">0.7572</td>
<td align="right">0.7813</td>
<td align="right">5.438</td>
<td align="right">94.44</td>
<td align="right">0.011490</td>
<td align="right">0.02461</td>
<td align="right">0.05688</td>
<td align="right">0.01885</td>
<td align="right">0.01756</td>
<td align="right">0.005115</td>
<td align="right">22.54</td>
<td align="right">16.67</td>
<td align="right">152.20</td>
<td align="right">1575.0</td>
<td align="right">0.1374</td>
<td align="right">0.2050</td>
<td align="right">0.4000</td>
<td align="right">0.1625</td>
<td align="right">0.2364</td>
<td align="right">0.07678</td>
</tr>
<tr class="even">
<td align="left">Malignant</td>
<td align="right">12.45</td>
<td align="right">15.70</td>
<td align="right">82.57</td>
<td align="right">477.1</td>
<td align="right">0.12780</td>
<td align="right">0.17000</td>
<td align="right">0.1578</td>
<td align="right">0.08089</td>
<td align="right">0.2087</td>
<td align="right">0.07613</td>
<td align="right">0.3345</td>
<td align="right">0.8902</td>
<td align="right">2.217</td>
<td align="right">27.19</td>
<td align="right">0.007510</td>
<td align="right">0.03345</td>
<td align="right">0.03672</td>
<td align="right">0.01137</td>
<td align="right">0.02165</td>
<td align="right">0.005082</td>
<td align="right">15.47</td>
<td align="right">23.75</td>
<td align="right">103.40</td>
<td align="right">741.6</td>
<td align="right">0.1791</td>
<td align="right">0.5249</td>
<td align="right">0.5355</td>
<td align="right">0.1741</td>
<td align="right">0.3985</td>
<td align="right">0.12440</td>
</tr>
</tbody>
</table>
<hr />
<!-- ## 3. Analyze the Correlation between variables -->
<p>## 2. Correlation and relationship between variables</p>
<!-- ### 3-1) Correlation between each variables {.tabset} -->
<!-- There are many ways to draw a correalation plot! -->
<!-- For practice, I applied different function to each data (mean, se, worst) -->
<!-- #### Mean -->
<!-- ```{r} -->
<!-- library(PerformanceAnalytics) -->
<!-- chart.Correlation(wbcd[,c(2:11)],histogram=TRUE, col="grey10", pch=1, main="Cancer Mean") -->
<!-- ``` -->
<!-- #### SE -->
<!-- ```{r} -->
<!-- library(psych) -->
<!-- pairs.panels(wbcd[,c(12:21)], method="pearson", -->
<!--              hist.col = "#1fbbfa", density=TRUE, ellipses=TRUE, show.points = TRUE, -->
<!--              pch=1, lm=TRUE, cex.cor=1, smoother=F, stars = T, main="Cancer SE") -->
<!-- ``` -->
<!-- #### Worst -->
<!-- ```{r} -->
<!-- library(ggplot2) -->
<!-- library(GGally) -->
<!-- ggpairs(wbcd[,c(22:31)],)+ theme_bw()+ -->
<!-- labs(title="Cancer Worst")+ -->
<!-- theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=13)) -->
<!-- ``` -->
<!-- ### 3-2) See the relation between each variables (diagnosis included) {.tabset} -->
<!-- I think viewing plot with diagnosis included is much more important than combined data[3-1]. -->
</div>
</div>
</div>
<div id="correlation-and-relationship-between-variables" class="section level2 tabset">
<h2>2. Correlation and relationship between variables</h2>
<div id="mean" class="section level3">
<h3>Mean</h3>
<p><img src="2_12_files/figure-html/unnamed-chunk-8-1.png" width="1152" /></p>
</div>
<div id="se" class="section level3">
<h3>SE</h3>
<p><img src="2_12_files/figure-html/unnamed-chunk-9-1.png" width="1152" /></p>
</div>
<div id="worst" class="section level3">
<h3>Worst</h3>
<p><img src="2_12_files/figure-html/unnamed-chunk-10-1.png" width="1152" /></p>
<p>high correlation value means it has “multicollinearity” between variables. Use one main component for model development by reduct the variables with high correlation.</p>
</div>
</div>
<div id="principal-component-analysis-pca" class="section level2">
<h2>3. Principal Component Analysis (PCA)</h2>
<!-- Too many variables can cause such problems below -->
<!-- - Increased computer throughput -->
<!-- - Too complex visualization problems -->
<!-- - Decrease efficiency by including variables that have no effect on the analysis -->
<!-- - Make data interpretation difficult -->
<!-- If you see the ggcorr plot above[3-3], high correlation value means it has "multicollinearity" between variables. -->
<!-- -> Use one main component for model development by reduct the variables with high correlation. -->
<!-- **When determining the number of principal components,  -->
<!-- use the cumulative contribution rate  -->
<!-- or use a screeplot and use the previous step of the principal component where the eigenvalue curve lies horizontally. -->
<!-- PCA uses standardized data so that it can avoid data distortion caused by scale difference. -->
<!-- ### 4-1) Summary {.tabset} -->
<!-- In the results of PCA, if the cumulative proportion is 85% or above, it can be determined by the number of principal components. -->
<!-- * View Point : Cumulative Proportion -->
<!-- For example, if cumulative proportion of PC4 is 88.7, it means **the sum of proportion of PC1~PC4** is 88.7! -->
<!-- #### All -->
<!-- The cumulative proportion from PC1 to PC6 is about 88.7%. (above 85%) -->
<!-- It means that PC1~PC6 can explain 88.7% of the whole data. -->
<!-- #### Mean -->
<!-- The cumulative proportion from PC1 to PC3 is about 88.7%. (above 85%) -->
<!-- #### SE -->
<!-- The cumulative proportion from PC1 to PC4 is about 86.7%. (above 85%) -->
<!-- #### Worst -->
<!-- The cumulative proportion from PC1 to PC3 is about 85.8%. (above 85%) -->
<!-- ### 4-2) Screeplot {.tabset} -->
<div id="screeplot" class="section level3 tabset">
<h3>3-1) Screeplot</h3>
<p>The percentage of variability explained by the principal components can be ascertained through screeplot.</p>
<!-- => View Point : principal components where the line lies. -->
<!-- #### All -->
<!-- Line lies at point PC6 -->
<p><img src="2_12_files/figure-html/unnamed-chunk-16-1.png" width="1152" /></p>
<!-- ### 4-3) Get PCA Variables {.tabset} -->
<!-- #### All -->
<!-- ##### Get PCA Variables -->
<div id="contributions-of-variables-to-pc1-pc2" class="section level5">
<h5>Contributions of variables to PC1 &amp; PC2</h5>
<p><img src="2_12_files/figure-html/unnamed-chunk-17-1.png" width="1152" /></p>
</div>
</div>
<div id="see-the-biplot" class="section level3 tabset">
<h3>3-2) See the Biplot</h3>
<p><img src="2_12_files/figure-html/unnamed-chunk-19-1.png" width="1152" /></p>
<hr />
</div>
</div>
<div id="apply-ml-methods-and-compare-each-other-and-choose-best-fits" class="section level2">
<h2>4. Apply ML methods and compare each other and choose best fits</h2>
<div id="make-test-train-dataset-for-testing-classification-ml-methods" class="section level3">
<h3>4-1) Make test &amp; train dataset for testing classification ML methods</h3>
<p>Shuffle the wbcd data(100%) &amp; Make train dataset(70%), test dataset(30%)</p>
<!-- ### 5-2) Check the proportion of diagnosis (Benign / Malignant) {.tabset} -->
<!-- #### train -->
<pre><code>## 
##    Benign Malignant 
## 0.6281407 0.3718593</code></pre>
<!-- #### test -->
<pre><code>## 
##    Benign Malignant 
##  0.625731  0.374269</code></pre>
</div>
<div id="apply-every-ml-methods-to-data" class="section level3 tabset">
<h3>4-2) Apply every ML methods to data</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)</code></pre></div>
<div id="rpart" class="section level4">
<h4>rpart</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rpart)
learn_rp &lt;-<span class="st"> </span><span class="kw">rpart</span>(diagnosis<span class="op">~</span>.,<span class="dt">data=</span>train,<span class="dt">control=</span><span class="kw">rpart.control</span>(<span class="dt">minsplit=</span><span class="dv">2</span>))
pre_rp &lt;-<span class="st"> </span><span class="kw">predict</span>(learn_rp, test[,<span class="op">-</span><span class="dv">1</span>], <span class="dt">type=</span><span class="st">&quot;class&quot;</span>)
cm_rp  &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(pre_rp, test<span class="op">$</span>diagnosis)
cm_rp</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  Benign Malignant
##   Benign       105         6
##   Malignant      2        58
##                                           
##                Accuracy : 0.9532          
##                  95% CI : (0.9099, 0.9796)
##     No Information Rate : 0.6257          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.8988          
##  Mcnemar&#39;s Test P-Value : 0.2888          
##                                           
##             Sensitivity : 0.9813          
##             Specificity : 0.9062          
##          Pos Pred Value : 0.9459          
##          Neg Pred Value : 0.9667          
##              Prevalence : 0.6257          
##          Detection Rate : 0.6140          
##    Detection Prevalence : 0.6491          
##       Balanced Accuracy : 0.9438          
##                                           
##        &#39;Positive&#39; Class : Benign          
## </code></pre>
</div>
<div id="randomforest" class="section level4">
<h4>randomForest</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(randomForest)
learn_rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(diagnosis<span class="op">~</span>., <span class="dt">data=</span>train, <span class="dt">ntree=</span><span class="dv">500</span>, <span class="dt">proximity=</span>T, <span class="dt">importance=</span>T)
pre_rf   &lt;-<span class="st"> </span><span class="kw">predict</span>(learn_rf, test[,<span class="op">-</span><span class="dv">1</span>])
cm_rf    &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(pre_rf, test<span class="op">$</span>diagnosis)
cm_rf</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  Benign Malignant
##   Benign       106         4
##   Malignant      1        60
##                                           
##                Accuracy : 0.9708          
##                  95% CI : (0.9331, 0.9904)
##     No Information Rate : 0.6257          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.937           
##  Mcnemar&#39;s Test P-Value : 0.3711          
##                                           
##             Sensitivity : 0.9907          
##             Specificity : 0.9375          
##          Pos Pred Value : 0.9636          
##          Neg Pred Value : 0.9836          
##              Prevalence : 0.6257          
##          Detection Rate : 0.6199          
##    Detection Prevalence : 0.6433          
##       Balanced Accuracy : 0.9641          
##                                           
##        &#39;Positive&#39; Class : Benign          
## </code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(learn_rf, <span class="dt">main=</span><span class="st">&quot;Random Forest (Error Rate vs. Number of Trees)&quot;</span>)</code></pre></div>
<p><img src="2_12_files/figure-html/unnamed-chunk-26-1.png" width="1152" /></p>
<!-- ##### Prediction Plot -->
<!-- I can't explain this plot exactly.  -->
<!-- Anybody who can describe this plot, please let me know. I'm happy to add in my kernel. -->
<p><img src="2_12_files/figure-html/unnamed-chunk-27-1.png" width="1152" /></p>
<div id="variance-importance-plot" class="section level5">
<h5>Variance Importance Plot</h5>
<ul>
<li>MeanDecreaseAccuracy : radius_worst &gt; concave.points_worst &gt; area_worst &gt; perimeter_worst</li>
</ul>
<p>Important parameters for accuracy improvement are determined by the “MeanDecreaseAccuracy”.</p>
<ul>
<li>MeanDecreaseGini : perimeter_worst &gt; radius_worst &gt; area_worst &gt; concave.points_worst</li>
</ul>
<p>Important parameters for improving node impurities are determined by the “MeanDecreaseGini”.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">varImpPlot</span>(learn_rf)</code></pre></div>
<p><img src="2_12_files/figure-html/unnamed-chunk-28-1.png" width="1152" /></p>
</div>
</div>
<div id="ctree" class="section level4">
<h4>ctree</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(party)
learn_ct &lt;-<span class="st"> </span><span class="kw">ctree</span>(diagnosis<span class="op">~</span>., <span class="dt">data=</span>train, <span class="dt">controls=</span><span class="kw">ctree_control</span>(<span class="dt">maxdepth=</span><span class="dv">2</span>))
pre_ct   &lt;-<span class="st"> </span><span class="kw">predict</span>(learn_ct, test[,<span class="op">-</span><span class="dv">1</span>])
cm_ct    &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(pre_ct, test<span class="op">$</span>diagnosis)
cm_ct</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  Benign Malignant
##   Benign        98         3
##   Malignant      9        61
##                                           
##                Accuracy : 0.9298          
##                  95% CI : (0.8806, 0.9632)
##     No Information Rate : 0.6257          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.8529          
##  Mcnemar&#39;s Test P-Value : 0.1489          
##                                           
##             Sensitivity : 0.9159          
##             Specificity : 0.9531          
##          Pos Pred Value : 0.9703          
##          Neg Pred Value : 0.8714          
##              Prevalence : 0.6257          
##          Detection Rate : 0.5731          
##    Detection Prevalence : 0.5906          
##       Balanced Accuracy : 0.9345          
##                                           
##        &#39;Positive&#39; Class : Benign          
## </code></pre>
</div>
<div id="knn---tune" class="section level4">
<h4>KNN - Tune</h4>
<div id="choose-k-which-shows-best-predict-performance-in-knn" class="section level5">
<h5>Choose ‘k’ which shows best predict performance in KNN</h5>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(class)

acc_test &lt;-<span class="st"> </span><span class="kw">numeric</span>()

<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">30</span>){
    predict &lt;-<span class="st"> </span><span class="kw">knn</span>(<span class="dt">train=</span>train[,<span class="op">-</span><span class="dv">1</span>], <span class="dt">test=</span>test[,<span class="op">-</span><span class="dv">1</span>], <span class="dt">cl=</span>train[,<span class="dv">1</span>], <span class="dt">k=</span>i, <span class="dt">prob=</span>T)
    acc_test &lt;-<span class="st"> </span><span class="kw">c</span>(acc_test,<span class="kw">mean</span>(predict<span class="op">==</span>test[,<span class="dv">1</span>]))
}

acc &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k=</span> <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">30</span>), <span class="dt">cnt =</span> acc_test)

opt_k &lt;-<span class="st"> </span><span class="kw">subset</span>(acc, cnt<span class="op">==</span><span class="kw">max</span>(cnt))[<span class="dv">1</span>,]
sub &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;Optimal number of k is&quot;</span>, opt_k<span class="op">$</span>k, <span class="st">&quot;(accuracy :&quot;</span>, opt_k<span class="op">$</span>cnt,<span class="st">&quot;) in KNN&quot;</span>)

<span class="kw">library</span>(highcharter)
<span class="co"># hchart(acc, &#39;line&#39;, hcaes(k, cnt)) %&gt;%</span>
<span class="co">#   hc_title(text = &quot;Accuracy With Varying K (KNN)&quot;) %&gt;%</span>
<span class="co">#   hc_subtitle(text = sub) %&gt;%</span>
<span class="co">#   hc_add_theme(hc_theme_google()) %&gt;%</span>
<span class="co">#   hc_xAxis(title = list(text = &quot;Number of Neighbors(k)&quot;)) %&gt;%</span>
<span class="co">#   hc_yAxis(title = list(text = &quot;Accuracy&quot;))</span></code></pre></div>
</div>
<div id="apply-optimal-k-to-show-best-predict-performance-in-knn" class="section level5">
<h5>Apply optimal K to show best predict performance in KNN</h5>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(class)
<span class="kw">library</span>(caret)
pre_knn &lt;-<span class="st"> </span><span class="kw">knn</span>(<span class="dt">train =</span> train[,<span class="op">-</span><span class="dv">1</span>], <span class="dt">test =</span> test[,<span class="op">-</span><span class="dv">1</span>], <span class="dt">cl =</span> train[,<span class="dv">1</span>], <span class="dt">k=</span>opt_k<span class="op">$</span>k, <span class="dt">prob=</span>T)
cm_knn  &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(pre_knn, test<span class="op">$</span>diagnosis)
cm_knn</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  Benign Malignant
##   Benign       106        10
##   Malignant      1        54
##                                           
##                Accuracy : 0.9357          
##                  95% CI : (0.8878, 0.9675)
##     No Information Rate : 0.6257          
##     P-Value [Acc &gt; NIR] : &lt; 2e-16         
##                                           
##                   Kappa : 0.8587          
##  Mcnemar&#39;s Test P-Value : 0.01586         
##                                           
##             Sensitivity : 0.9907          
##             Specificity : 0.8438          
##          Pos Pred Value : 0.9138          
##          Neg Pred Value : 0.9818          
##              Prevalence : 0.6257          
##          Detection Rate : 0.6199          
##    Detection Prevalence : 0.6784          
##       Balanced Accuracy : 0.9172          
##                                           
##        &#39;Positive&#39; Class : Benign          
## </code></pre>
</div>
</div>
<div id="k-means" class="section level4">
<h4>K-Means</h4>
<div id="make-kmeans-predict-function" class="section level5">
<h5>Make KMEANS predict function</h5>
<p>we have to make function to predict using kmeans methods, since orgin predict function don’t support kmeans.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">predict.kmeans &lt;-<span class="st"> </span><span class="cf">function</span>(newdata, object){
    centers &lt;-<span class="st"> </span>object<span class="op">$</span>centers
    n_centers &lt;-<span class="st"> </span><span class="kw">nrow</span>(centers)
    dist_mat &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">dist</span>(<span class="kw">rbind</span>(centers, newdata)))
    dist_mat &lt;-<span class="st"> </span>dist_mat[<span class="op">-</span><span class="kw">seq</span>(n_centers), <span class="kw">seq</span>(n_centers)]
    <span class="kw">max.col</span>(<span class="op">-</span>dist_mat)
}</code></pre></div>
</div>
<div id="apply-kmeans" class="section level5">
<h5>apply kmeans</h5>
<p>you have to apply centers to 2, since there are only two factors(benign, malignant)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># library(caret)</span>
<span class="co"># learn_kmeans &lt;- kmeans(train[,-1], centers=2)</span>
<span class="co">#</span>
<span class="co"># pre_kmeans &lt;- predict.kmeans(test[,-1],learn_kmeans)</span>
<span class="co"># pre_kmeans &lt;- ifelse(pre_kmeans == 1,&quot;Benign&quot;,&quot;Malignant&quot;)</span>
<span class="co"># cm_kmeans &lt;- confusionMatrix(pre_kmeans, test$diagnosis)</span>
<span class="co"># length(pre_kmeans)==length(test$diagnosis)</span>
<span class="co"># str(test$diagnosis)</span>
<span class="co"># cm_kmeans</span></code></pre></div>
</div>
<div id="plot" class="section level5">
<h5>plot</h5>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(factoextra)
<span class="co"># learn_kmeans$cluster &lt;- ifelse(learn_kmeans$cluster == 1,&quot;Benign&quot;,&quot;Malignant&quot;)</span>
<span class="co"># fviz_cluster(learn_kmeans, data = train[,-1])</span></code></pre></div>
</div>
</div>
<div id="gbm" class="section level4">
<h4>GBM</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(gbm)
test_gbm &lt;-<span class="st"> </span><span class="kw">gbm</span>(diagnosis<span class="op">~</span>., <span class="dt">data=</span>train, <span class="dt">distribution=</span><span class="st">&quot;gaussian&quot;</span>,<span class="dt">n.trees =</span> <span class="dv">10000</span>,
                <span class="dt">shrinkage =</span> <span class="fl">0.01</span>, <span class="dt">interaction.depth =</span> <span class="dv">4</span>, <span class="dt">bag.fraction=</span><span class="fl">0.5</span>, <span class="dt">train.fraction=</span><span class="fl">0.5</span>,<span class="dt">n.minobsinnode=</span><span class="dv">10</span>,<span class="dt">cv.folds=</span><span class="dv">3</span>,<span class="dt">keep.data=</span><span class="ot">TRUE</span>,<span class="dt">verbose=</span><span class="ot">FALSE</span>,<span class="dt">n.cores=</span><span class="dv">1</span>)
best.iter &lt;-<span class="st"> </span><span class="kw">gbm.perf</span>(test_gbm, <span class="dt">method=</span><span class="st">&quot;cv&quot;</span>,<span class="dt">plot.it=</span><span class="ot">FALSE</span>)</code></pre></div>
<p><img src="2_12_files/figure-html/unnamed-chunk-35-1.png" width="1152" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fitControl =<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;cv&quot;</span>, <span class="dt">number=</span><span class="dv">5</span>, <span class="dt">returnResamp=</span><span class="st">&quot;all&quot;</span>)
learn_gbm =<span class="st"> </span><span class="kw">train</span>(diagnosis<span class="op">~</span>., <span class="dt">data=</span>train, <span class="dt">method=</span><span class="st">&quot;gbm&quot;</span>, <span class="dt">distribution=</span><span class="st">&quot;bernoulli&quot;</span>, <span class="dt">trControl=</span>fitControl, <span class="dt">verbose=</span>F, <span class="dt">tuneGrid=</span><span class="kw">data.frame</span>(<span class="dt">.n.trees=</span>best.iter, <span class="dt">.shrinkage=</span><span class="fl">0.01</span>, <span class="dt">.interaction.depth=</span><span class="dv">1</span>, <span class="dt">.n.minobsinnode=</span><span class="dv">1</span>))
pre_gbm &lt;-<span class="st"> </span><span class="kw">predict</span>(learn_gbm, test[,<span class="op">-</span><span class="dv">1</span>])
cm_gbm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(pre_gbm, test<span class="op">$</span>diagnosis)
cm_gbm</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  Benign Malignant
##   Benign       106         5
##   Malignant      1        59
##                                          
##                Accuracy : 0.9649         
##                  95% CI : (0.9252, 0.987)
##     No Information Rate : 0.6257         
##     P-Value [Acc &gt; NIR] : &lt;2e-16         
##                                          
##                   Kappa : 0.9241         
##  Mcnemar&#39;s Test P-Value : 0.2207         
##                                          
##             Sensitivity : 0.9907         
##             Specificity : 0.9219         
##          Pos Pred Value : 0.9550         
##          Neg Pred Value : 0.9833         
##              Prevalence : 0.6257         
##          Detection Rate : 0.6199         
##    Detection Prevalence : 0.6491         
##       Balanced Accuracy : 0.9563         
##                                          
##        &#39;Positive&#39; Class : Benign         
## </code></pre>
</div>
<div id="svm" class="section level4">
<h4>SVM</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(e1071)
learn_svm &lt;-<span class="st"> </span><span class="kw">svm</span>(diagnosis<span class="op">~</span>., <span class="dt">data=</span>train)
pre_svm &lt;-<span class="st"> </span><span class="kw">predict</span>(learn_svm, test[,<span class="op">-</span><span class="dv">1</span>])
cm_svm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(pre_svm, test<span class="op">$</span>diagnosis)
cm_svm</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  Benign Malignant
##   Benign       106         3
##   Malignant      1        61
##                                           
##                Accuracy : 0.9766          
##                  95% CI : (0.9412, 0.9936)
##     No Information Rate : 0.6257          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.9497          
##  Mcnemar&#39;s Test P-Value : 0.6171          
##                                           
##             Sensitivity : 0.9907          
##             Specificity : 0.9531          
##          Pos Pred Value : 0.9725          
##          Neg Pred Value : 0.9839          
##              Prevalence : 0.6257          
##          Detection Rate : 0.6199          
##    Detection Prevalence : 0.6374          
##       Balanced Accuracy : 0.9719          
##                                           
##        &#39;Positive&#39; Class : Benign          
## </code></pre>
</div>
<div id="svm---tune" class="section level4">
<h4>SVM - Tune</h4>
<div id="choose-gamma-cost-which-shows-best-predict-performance-in-svm" class="section level5">
<h5>Choose ‘gamma, cost’ which shows best predict performance in SVM</h5>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gamma &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="fl">0.1</span>,<span class="fl">0.005</span>)
cost &lt;-<span class="st"> </span><span class="dv">2</span><span class="op">^</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">5</span>)
parms &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">cost=</span>cost, <span class="dt">gamma=</span>gamma)    ## 231

acc_test &lt;-<span class="st"> </span><span class="kw">numeric</span>()
accuracy1 &lt;-<span class="st"> </span><span class="ot">NULL</span>; accuracy2 &lt;-<span class="st"> </span><span class="ot">NULL</span>

<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">NROW</span>(parms)){
        learn_svm &lt;-<span class="st"> </span><span class="kw">svm</span>(diagnosis<span class="op">~</span>., <span class="dt">data=</span>train, <span class="dt">gamma=</span>parms<span class="op">$</span>gamma[i], <span class="dt">cost=</span>parms<span class="op">$</span>cost[i])
        pre_svm &lt;-<span class="st"> </span><span class="kw">predict</span>(learn_svm, test[,<span class="op">-</span><span class="dv">1</span>])
        accuracy1 &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(pre_svm, test<span class="op">$</span>diagnosis)
        accuracy2[i] &lt;-<span class="st"> </span>accuracy1<span class="op">$</span>overall[<span class="dv">1</span>]
}

acc &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">p=</span> <span class="kw">seq</span>(<span class="dv">1</span>,<span class="kw">NROW</span>(parms)), <span class="dt">cnt =</span> accuracy2)

opt_p &lt;-<span class="st"> </span><span class="kw">subset</span>(acc, cnt<span class="op">==</span><span class="kw">max</span>(cnt))[<span class="dv">1</span>,]
sub &lt;-<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;Optimal number of parameter is&quot;</span>, opt_p<span class="op">$</span>p, <span class="st">&quot;(accuracy :&quot;</span>, opt_p<span class="op">$</span>cnt,<span class="st">&quot;) in SVM&quot;</span>)

<span class="kw">library</span>(highcharter)
<span class="kw">library</span>(magrittr)
<span class="co"># hchart(acc, &#39;line&#39;, hcaes(p, cnt)) %&gt;%</span>
<span class="co">#   hc_title(text = &quot;Accuracy With Varying Parameters (SVM)&quot;) %&gt;%</span>
<span class="co">#   hc_subtitle(text = sub) %&gt;%</span>
<span class="co">#   hc_add_theme(hc_theme_google()) %&gt;%</span>
<span class="co">#   hc_xAxis(title = list(text = &quot;Number of Parameters&quot;)) %&gt;%</span>
<span class="co">#   hc_yAxis(title = list(text = &quot;Accuracy&quot;))</span></code></pre></div>
</div>
<div id="apply-optimal-parametersgamma-cost-to-show-best-predict-performance-in-svm" class="section level5">
<h5>Apply optimal parameters(gamma, cost) to show best predict performance in SVM</h5>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(e1071)
<span class="kw">library</span>(caret)
learn_imp_svm &lt;-<span class="st"> </span><span class="kw">svm</span>(diagnosis<span class="op">~</span>., <span class="dt">data=</span>train, <span class="dt">cost=</span>parms<span class="op">$</span>cost[opt_p<span class="op">$</span>p], <span class="dt">gamma=</span>parms<span class="op">$</span>gamma[opt_p<span class="op">$</span>p])
pre_imp_svm &lt;-<span class="st"> </span><span class="kw">predict</span>(learn_imp_svm, test[,<span class="op">-</span><span class="dv">1</span>])
cm_imp_svm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(pre_imp_svm, test<span class="op">$</span>diagnosis)
cm_imp_svm</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##            Reference
## Prediction  Benign Malignant
##   Benign       107         2
##   Malignant      0        62
##                                           
##                Accuracy : 0.9883          
##                  95% CI : (0.9584, 0.9986)
##     No Information Rate : 0.6257          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.9749          
##  Mcnemar&#39;s Test P-Value : 0.4795          
##                                           
##             Sensitivity : 1.0000          
##             Specificity : 0.9688          
##          Pos Pred Value : 0.9817          
##          Neg Pred Value : 1.0000          
##              Prevalence : 0.6257          
##          Detection Rate : 0.6257          
##    Detection Prevalence : 0.6374          
##       Balanced Accuracy : 0.9844          
##                                           
##        &#39;Positive&#39; Class : Benign          
## </code></pre>
</div>
</div>
</div>
<div id="visualize-to-compare-the-accuracy-of-all-methods" class="section level3">
<h3>4-3) Visualize to compare the accuracy of all methods</h3>
<p><img src="2_12_files/figure-html/unnamed-chunk-39-1.png" width="1152" /></p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>5. Conclusion</h2>
<p>Select a best prediction model according to high accuracy</p>
<pre><code>##   imp_svm 
## 0.9883041</code></pre>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
